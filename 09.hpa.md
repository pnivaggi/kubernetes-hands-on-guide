# Horizontal Pod Autoscaler

For Kubernetes, the Metrics API offers a basic set of metrics to support automatic scaling and similar use cases. This API makes information available about resource usage for node and pod, including metrics for CPU and memory. If you deploy the Metrics API into your cluster, clients of the Kubernetes API can then query for this information, and you can use Kubernetes' access control mechanisms to manage permissions to do so.

## Task 1. Install Metrics server on Kind

Execute the following command to define the configuration patch required for Kind cluster:

```bash
cat > metric-server-patch.yaml << EOF 
spec:
  template:
    spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        - --kubelet-insecure-tls
        name: metrics-server 
EOF
```

Install Metrics server and configure it with the patched config:

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yaml
kubectl patch deployment metrics-server -n kube-system --patch "$(cat metric-server-patch.yaml)" 
```

## Task 2. Check Metrics API availability

When the Metrics server is up and running you can use the `kubectl top` command to get resource utilization per node or Pod.

Check Metrics server is up:

```bash
kubectl get pods -n kube-system | grep metrics
```

Make sure the Pod is running:

```console
kubectl get pods -n kube-system | grep metrics
metrics-server-6cb89dbdcf-qktvv              1/1     Running   0          98m
```

Run the following command to check you have access to the Metrics API:

```bash
kubectl top nodes
```

```console
kubectl top nodes
NAME                 CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
demo-control-plane   213m         4%     1031Mi          13%       
demo-worker          340m         6%     846Mi           10%       
demo-worker2         420m         8%     770Mi           9% 
```

```bash
kubectl top pods 
```

```console
eti-lab> kubectl top pods 
No resources found in default namespace.
```

## Taks 3. Run and expose php-apache server

To demonstrate a HorizontalPodAutoscaler, you will first start a Deployment that runs a container using the hpa-example image, and expose it as a Service using the following manifest:

```bash
cat > php-apache-server.yaml << EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  selector:
    matchLabels:
      run: php-apache
  replicas: 1
  template:
    metadata:
      labels:
        run: php-apache
    spec:
      containers:
      - name: php-apache
        image: registry.k8s.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
  labels:
    run: php-apache
spec:
  ports:
  - port: 80
  selector:
    run: php-apache
EOF
```

Then execute the following command to deploy the `php-apache` server:

```bash
kubectl apply -f php-apache-server.yaml
```

You should have an output similar to:

```console
kubectl apply -f php-apache-server.yaml
deployment.apps/php-apache created
service/php-apache created
```

## Task 4. Create the HorizontalPodAutoscaler

Now that the server is running, create the autoscaler using kubectl. There is kubectl autoscale subcommand, part of kubectl, that helps you do this.

You will shortly run a command that creates a HorizontalPodAutoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache Deployment that you created in the first step of these instructions.

Roughly speaking, the HPA controller will increase and decrease the number of replicas (by updating the Deployment) to maintain an average CPU utilization across all Pods of 50%. The Deployment then updates the ReplicaSet - this is part of how all Deployments work in Kubernetes - and then the ReplicaSet either adds or removes Pods based on the change to its .spec.

Since each pod requests 200 milli-cores by kubectl run, this means an average CPU usage of 100 milli-cores. 

Create the HorizontalPodAutoscaler:

```bash
kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
```

You can check the current status of the newly-made HorizontalPodAutoscaler, by running:

```bash
kubectl get hpa
```

```console
kubectl get hpa
NAME         REFERENCE               TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   <unknown>/50%   1         10        1          5m57s
```

## Task 5. Increase the load

Next, see how the autoscaler reacts to increased load. To do this, you'll start a different Pod to act as a client. The container within the client Pod runs in an infinite loop, sending queries to the php-apache service.

Execute the following command:

```bash
kubectl run load-generator --image=busybox:1.28 --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"
```

Then run the following command to check the impact on the number of replicas adjustement:

```bash
kubectl get hpa php-apache --watch
```

```console
kubectl get hpa --watch
NAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   225%/50%   1         10        1          4h47m
php-apache   Deployment/php-apache   248%/50%   1         10        4          4h48m
php-apache   Deployment/php-apache   85%/50%    1         10        5          4h48m
php-apache   Deployment/php-apache   38%/50%    1         10        5          4h48m
php-apache   Deployment/php-apache   55%/50%    1         10        5          4h48m
php-apache   Deployment/php-apache   46%/50%    1         10        5          4h49m
php-apache   Deployment/php-apache   55%/50%    1         10        5          4h49m
php-apache   Deployment/php-apache   52%/50%    1         10        6          4h49m
...
php-apache   Deployment/php-apache   42%/50%    1         10        6          4h58m
php-apache   Deployment/php-apache   58%/50%    1         10        6          4h58m
php-apache   Deployment/php-apache   44%/50%    1         10        7          4h58m
```

You can exit the watch command with `CTRL^C` break command 

Check the deployment for `php-apache` server with the following command:

```bash
kubectl get deployment php-apache
```

After K8s has converged (~ 10min) you should have a similar output:

```console
kubectl get deployment php-apache
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
php-apache   7/7     7            7           4h39m
```

## Task 6. Stop generating load

To finish the example, stop sending the load:

```bash
kubectl delete pod load-generator
```

Then verify the result state (after a minute or so):

```bash
kubectl get hpa php-apache --watch
```

The output is similar to:

```console
pnivaggi@PNIVAGGI-M-TQ6R kind-local % kubectl get hpa --watch
NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   0%/50%    1         10        7          4h37m
php-apache   Deployment/php-apache   0%/50%    1         10        6          4h38m
php-apache   Deployment/php-apache   0%/50%    1         10        5          4h38m
php-apache   Deployment/php-apache   0%/50%    1         10        5          4h38m
php-apache   Deployment/php-apache   0%/50%    1         10        1          4h39m
```

Once CPU utilization dropped to 0, the HPA automatically scaled the number of replicas back down to 1.

Autoscaling the replicas may take a few minutes.

Check the deployment is configured with replicas=1 when the system has converged (~ 3min):

```bash
kubectl get deployment php-apache
```

The output is similar to:

```console
kubectl get deployment php-apache
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
php-apache   1/1     1            1           6h2m
```

## Task 7. Autoscaling on multiple metrics and custom metrics

You can introduce additional metrics to use when autoscaling the php-apache Deployment by making use of the autoscaling/v2 API version.

First, define the YAML of your HorizontalPodAutoscaler in the autoscaling/v2 form:

```bash
cat > hpa-php-apache.yaml <<EOF
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  - type: Pods # Pod metrics - averaged together accross Pods
    pods:
      metric:
        name: packets-per-second
      target:
        type: AverageValue
        averageValue: 1k
  - type: Object # Object metrics
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        name: main-route
      target:
        type: Value
        value: 10k
EOF
```

There are two other types of metrics, both of which are considered custom metrics: pod metrics and object metrics. 

The first of these alternative metric types is pod metrics. These metrics describe Pods, and are averaged together across Pods and compared with a target value to determine the replica count. They work much like resource metrics, except that they only support a target type of AverageValue.

The second alternative metric type is object metrics. These metrics describe a different object in the same namespace, instead of describing Pods. The metrics are not necessarily fetched from the object; they only describe it. Object metrics support target types of both Value and AverageValue. With Value, the target is compared directly to the returned metric from the API. With AverageValue, the value returned from the custom metrics API is divided by the number of Pods before being compared to the target.

Run the following command to deploy the hpa:

```bash
kubectl apply -f hpa-php-apache.yaml
```

Execute the following command to check `php-apache` hpa configuration, conditions and events:

```bash
kubectl describe hpa php-apache
```

The ouput is similar to:

```console
kubectl describe hpa php-apache
Warning: autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+; use autoscaling/v2 HorizontalPodAutoscaler
Name:                                                          php-apache
Namespace:                                                     default
Labels:                                                        <none>
Annotations:                                                   <none>
CreationTimestamp:                                             Mon, 24 Oct 2022 15:58:10 +0200
Reference:                                                     Deployment/php-apache
Metrics:                                                       ( current / target )
  "packets-per-second" on pods:                                <unknown> / 1k
  "requests-per-second" on Ingress/main-route (target value):  <unknown> / 10k
  resource cpu on pods  (as a percentage of request):          45% (91m) / 50%
Min replicas:                                                  1
Max replicas:                                                  10
Deployment pods:                                               7 current / 7 desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    SucceededGetScale    the HPA controller was able to get the target's current scale
  ScalingActive   False   FailedGetPodsMetric  the HPA was unable to compute the replica count: unable to get metric packets-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered
  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range
Events:
  Type     Reason                        Age                   From                       Message
  ----     ------                        ----                  ----                       -------
  Warning  FailedComputeMetricsReplicas  8m17s (x4 over 9m2s)  horizontal-pod-autoscaler  invalid metrics (2 invalid out of 3), first error is: failed to get pods metric value: unable to get metric packets-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered
  Normal   SuccessfulRescale             8m2s                  horizontal-pod-autoscaler  New size: 3; reason: cpu resource utilization (percentage of request) above target
  Normal   SuccessfulRescale             7m47s                 horizontal-pod-autoscaler  New size: 5; reason: cpu resource utilization (percentage of request) above target
  Warning  FailedGetObjectMetric         7m2s (x9 over 9m2s)   horizontal-pod-autoscaler  unable to get metric requests-per-second: Ingress on default main-route/unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered
  Warning  FailedGetPodsMetric           4m2s (x21 over 9m2s)  horizontal-pod-autoscaler  unable to get metric packets-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered
```

Notice `unable to get metric requests-per-second` and `unable to get metric packets-per-second` warning messages. These metrics are not fetched by the Metrics server, and require a more advanced cluster monitoring setup like with Prometheus.